{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install llama-index\n!pip install llama-index-llms-groq\n!pip install PyPDF2\n!pip install llama-index-packs-raft-dataset\n!pip install llama-index-embeddings-huggingface\n!pip install llama-index-embeddings-instructor\n!pip install pymupdf\n!pip install --upgrade pip\n!pip install ipywidgets==8.1.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_index.packs.raft_dataset import RAFTDatasetPack\nfrom llama_index.llms.groq import Groq\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nimport PyPDF2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the file paths for input and output\npdf_files = [\n    '/kaggle/input/testing-manual/Circuit Breaker Testing Manual.pdf',\n    '/kaggle/input/testing-manual/Power Cable Testing Manual.pdf',\n    '/kaggle/input/testing-manual/Transformer Testing Manual.pdf'\n]\noutput_path = '/kaggle/working/Merged_Testing_Manuals.pdf'\n\n# Create a PDF writer object to hold the merged content\npdf_writer = PyPDF2.PdfWriter()\n\n# Loop through each PDF file and add its pages to the writer\nfor pdf_file in pdf_files:\n    pdf_reader = PyPDF2.PdfReader(pdf_file)\n    for page in range(len(pdf_reader.pages)):\n        pdf_writer.add_page(pdf_reader.pages[page])\n\n# Write the combined PDF to the output path\nwith open(output_path, 'wb') as output_pdf:\n    pdf_writer.write(output_pdf)\n\nprint(\"PDF files merged successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llm = Groq(model=\"qwen-2.5-32b\", api_key=\"\")\nembeddings = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raft_dataset = RAFTDatasetPack(file_path=\"/kaggle/working/Merged_Testing_Manuals.pdf\",\n                                 llm=llm,\n                                 num_questions_per_chunk=1,\n                                 num_distract_docs=2,\n                                 embed_model = embeddings,\n                                 )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunks = raft_dataset.get_chunks(\"/kaggle/working/Merged_Testing_Manuals.pdf\", raft_dataset.chunk_size)\nlen(chunks)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = raft_dataset.run()","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset[0]['context']['sentences'][0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_path = \"/kaggle/working/RAFTDATA\"\n# Save as .arrow format\ndataset.save_to_disk(output_path)\n\n# Save as .jsonl format\ndataset.to_json(output_path + \".jsonl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Define the directory to be zipped and the output zip file path\ndirectory_to_zip = '/kaggle/working/RAFTDATA'\nzip_file_path = '/kaggle/working/RAFTDATA.zip'\n\n# Check if the directory exists\nif os.path.exists(directory_to_zip):\n    # Create a zip file\n    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk the directory and add files to the zip file\n        for root, dirs, files in os.walk(directory_to_zip):\n            for file in files:\n                # Create the complete file path\n                file_path = os.path.join(root, file)\n                # Add the file to the zip file\n                zipf.write(file_path, os.path.relpath(file_path, directory_to_zip))\n    \n    print(f\"Directory '{directory_to_zip}' has been zipped successfully as '{zip_file_path}'.\")\nelse:\n    print(f\"The directory '{directory_to_zip}' does not exist.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Path to the JSONL file\nfile_path = \"/kaggle/working/RAFTDATA.jsonl\"\n\n# Read the JSONL file into a DataFrame\ndf = pd.read_json(file_path, lines=True)\n\n# Show the first few rows\nprint(df.head())\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save as a CSV\ndf.to_csv(\"/kaggle/working/RAFTDATA.csv\", index=False)\n\n# Save as a JSON\ndf.to_json(\"/kaggle/working/RAFTDATA.json\", orient=\"records\", lines=True)\n\n# Save as a Parquet file\ndf.to_parquet(\"/kaggle/working/RAFTDATA.parquet\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop columns where all values are NaN\ndf_clean = df.dropna(axis=1, how='all')\n\n# Save the cleaned DataFrame to a CSV\ndf_clean.to_csv(\"/kaggle/working/RAFTDATA_clean.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.iloc[:, 2:]  # Removes the first two columns, keeps everything from the third column onward\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display\n\n# Display the DataFrame as a table\ndisplay(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.core.display import HTML\n\n# Convert to HTML and display\nHTML(df.to_html())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}